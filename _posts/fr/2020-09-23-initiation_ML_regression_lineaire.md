---
layout: post
title: "Initiation au Machine Learning - La Regr√©ssion Lin√©aire"
excerpt: "Il existe plusieurs types de machine learning, comme l'apprentissage supervis√©, l'apprentissage non-supervis√© et l'apprentissage semi-supervis√©. Chacun utilise des techniques diff√©rentes pour √©tablir une pr√©diction, mais le choix d'une m√©thode d√©pendra surtout du format de la donn√©e. Aujourd'hui nous nous attarderons seulement sur l'apprentissage supervis√© et sur un mod√®le en particulier: la R√©gression Lin√©aire."
lang: fr
permalink: /fr/initiation-machine-learning-regression-lineaire/
authors:
    - HugoDurand
date: '2020-09-23 10:42:47 +0100'
date_gmt: '2017-09-23 09:42:47 +0100'
---

Premi√®rement, prenons quelques instants pour faire une br√®ve introduction sur ce qu'est le machine learning.

Partons de la d√©finition de <a href="https://fr.wikipedia.org/wiki/Arthur_Samuel">Arthur Samuel</a> : "Field of study that gives computers the ability to learn without being explicitly programmed".

Traduction (by Google) : "Domaine d'√©tudes qui donne aux ordinateurs la possibilit√© d'apprendre sans √™tre explicitement programm√©"

En effet le machine learning, faisant partie du concept plus large d'intelligence artificielle, consiste √† cr√©er des algorithmes "auto-apprenants" en se basant sur de la donn√©e, ou une exp√©rience.

Prenons les deux sch√©mas ci-dessous :

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/schema-process.png" alt="SCHEMA-PROCESS" width="800">

Un algorithme r√©pond le plus souvent √† une probl√©matique gr√¢ce √† une suite d'instructions/r√®gles qu'on lui donne.
Le machine learning d√©finit donc la capacit√© d'une machine √† r√©pondre √† une probl√©matique sans qu'on ne d√©finisse les r√®gles.
Le r√©sultat sera une pr√©diction obtenue gr√¢ce √† un mod√®le bas√© sur des donn√©es et qui aura aussi la capacit√© de s'am√©liorer avec l'exp√©rience.

Comme l'a d√©fini <a href="https://fr.wikipedia.org/wiki/Tom_M._Mitchell">Tom Mitchell</a> : "A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E."

Traduction : " Un programme informatique est cens√© apprendre d'une exp√©rience E par rapport √† une t√¢che T et une certaine mesure de performance P, si ses performances sur T, mesur√©es par P, s'am√©liorent avec exp√©rience E."

<br/>

Partant de cette introduction, il existe plusieurs types de machine learning comme l'apprentissage supervis√©, l'apprentissage non-supervis√© et l'apprentissage semi-supervis√©.

Chacun utilise des techniques diff√©rentes pour √©tablir une pr√©diction, mais le choix d'une m√©thode d√©pendra surtout du format de la donn√©e.

Aujourd'hui, dans le cadre de cette initiation, nous nous attarderons seulement sur l'apprentissage supervis√© et sur un mod√®le en particulier : la R√©gression Lin√©aire.

On parle d'apprentissage supervis√© lorsque la donn√©e utilis√©e est labellis√©e.
Les variables de sorties, c'est √† dire les r√©ponses possibles, sont d√©j√† d√©finies et l'algorithme apportera "la bonne r√©ponse".
Le but est de trouver une fonction qui fait le lien entre les variables d'entr√©e X et la variable √† pr√©dire Y tel que Y = f(X)

On appelle cela un mod√®le de pr√©diction.

La variable de sortie Y peut √™tre de deux types, continue ou discr√®te.
Discr√®te, c'est √† dire une valeur finie que l'on peut √©num√©rer (1,2,3, Vrai, Faux).
Continue, c'est √† dire qui prend n'importe quelle valeur qui est d√©finie dans un intervalle (entre 0 et 300 secondes ou entre 10 et 20 euros, etc.).
La pr√©diction de variables continues concerne le plus souvent les mod√®les de r√©gression, par exemple pour pr√©dire le prix d'une maison, le poids d'un humain, la taille, etc. Pour les variables discr√®tes, elles rel√®vent plus de la classification, qui est un autre type d'apprentissage supervis√© dont le but est de pr√©dire si la donn√©e d'entr√©e appartient √† une cat√©gorie.

# La r√©gression Lin√©aire

Il existe plusieurs types de r√©gression. Il y a la r√©gression lin√©aire, la polynomiale, support vector, decision tree, random forest, etc.

Dans le cadre d'un exemple simple nous resterons sur la r√©gression lin√©aire.

Mais dans un premier temps, voyons comment il est possible de faire une pr√©diction.

Le but sera de donner une estimation sur une nouvelle donn√©e qui n'est pas encore connue par l'algorithme.

Mettons que nous voulons pr√©dire le prix d'un appartement bas√© sur une variable qui est sa taille, et que nous avons d√©j√† quelques donn√©es existantes et exactes sur lesquelles s'appuyer pour faire cette pr√©diction

N.B. : Afin d'illustrer l'exemple, nous allons commencer par la repr√©sentation graphique d'une fonction lin√©aire.

Ce graphique n'est pas forc√©ment exact, mais il illustrera parfaitement ce que l'on veut expliquer dans notre cas. Le but dans un premier temps est d'en comprendre la logique globale, nous expliquerons le calcul par la suite.

Nous avons donc ce graphique :

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/graphique-vide.png" alt="GRAPHIQUE-VIDE" width="800">

sur lequel nous allons pouvoir placer nos donn√©es d'exemple qui correspondent aux prix d'autres appartements par rapport √† leur taille.

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/graphique-donnees.png" alt="GRAPHIQUE-DONNEES" width="800">

Pour faire nos pr√©dictions nous allons mod√©liser le prix au m2 avec une fonction lin√©aire.

D√©finition :

Une fonction lin√©aire est une fonction qui traduit une situation de proportionnalit√©. Soit ***a*** un nombre r√©el quelconque. La fonction lin√©aire de coefficient ***a*** est la fonction qui associe √† tout nombre r√©el *x* le produit *a* √ó *x*.

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/calcul-f-lineaire.png" alt="CALCUL-F-LINEAIRE" width="150">


Elle est repr√©sent√©e sur le sch√©ma par une droite qui passe au milieu de ces points de mani√®re √† ce que la distance entre chaque point et la droite soit la plus faible possible.

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/graphique-lineaire.png" alt="GRAPHIQUE-LINEAIRE" width="800">

Avec cette hypoth√®se, nous pouvons donc pr√©dire - visuellement tout du moins - qu'un appartement de **40m¬≤** couterait un peu moins de **250k**‚Ç¨

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/graphique-prediction.png" alt="GRAPHIQUE-PREDICTION" width="800">

Mais comme dit pr√©c√©demment ce n'est que la repr√©sentation graphique du mod√®le de pr√©diction, et cela ne nous explique pas r√©ellement comment on peut pr√©dire le prix d'un appartement de 40m2.

D'un point de vue un peu plus "math√©matique", notre mod√®le de pr√©diction aura deux param√®tres &theta;<sub>0</sub> et &theta;<sub>1</sub>.

Par convention, la lettre Grecque Œ∏ (th√™ta) est fr√©quemment utilis√©e pour repr√©senter un param√®tre du mod√®le.

C'est en changeant ces param√®tres que nous pouvons ajuster notre fonction lin√©aire.

Le premier repr√©sente la valeur sur laquelle la droite croise l'axe des abscisses, soit **0** dans notre cas, et le deuxi√®me repr√©sente le **slope**, c'est √† dire le coefficient avec lequel la droite est influ√©e par la premi√®re valeur.

Pour calculer le coefficient de cette droite, nous pouvons prendre deux points sur la courbe et faire le calcul :

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/calcul-slope.png" alt="CALCUL-SLOPE" width="180">

prenons ces deux points :

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/graphique-slope.png" alt="GRAPHIQUE-SLOPE" width="800">

nous avons un premier point avec **10** en abscisse et **50** en ordonn√©e (environ) et un deuxi√®me en **40** et **250**.

le slope est donc √©gal √† :

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/detail-calcul-slope.png" alt="DETAIL-CALCUL-SLOPE" width="180">

Nous pouvons maintenant calculer une pr√©diction.

L'hypoth√®se (qui se note  h<sub>&theta;</sub>(x)) pour pr√©dire une nouvelle valeur se base donc sur ces deux param√®tres tels que :

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/calcul-prediction.png" alt="CALCUL-PREDICTION" width="230">

soit :

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/detail-calcul-prediction.png" alt="DETAIL-CALCUL-PREDICTION" width="230">

Selon notre mod√®le de pr√©diction, un appartement de **40m2** co√ªterait donc **264k‚Ç¨**.
Ce qui n'est pas tr√®s loin de ce que nous avions observ√© √† l'oeil sur la droite, mais pas exact non plus.

√Ä noter que dans notre cas la droite part de z√©ro, mais ce n'est pas toujours le cas.

Nous aurions tr√®s bien pu avoir des donn√©es dispos√©es diff√©remment et donc une droite avec une position diff√©rente :

<![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/graphique-f-lineaire-2.png" alt="GRAPHIQUE-F-LINEAIRE-2" width="800">

Dans ce cas &theta; aurait √©t√© √©gal √† **90** et non pas **0**.

√Ä noter que le coefficient n'aurait pas √©t√© le m√™me non plus.

Je vous invite √† recommencer l'exercice avec cet exemple.

La question qu'on peut se poser maintenant est comment fait-on sans graphique ?

Comment la machine fait pour conna√Ætre les coefficients sans placer visuellement deux points sur la courbe ?

Justement c'est l√† qu'on en revient √† la r√©gression lin√©aire. Son but n'est pas seulement la pr√©diction finale, mais de trouver et ajuster les param√®tres qui permettront √† la fonction lin√©aire de correspondre au mieux √† nos donn√©es et de faire les meilleures pr√©dictions.

Pour cela nous avons besoin de deux choses, une fonction de co√ªt et une descente de gradient.

**Fonction de co√ªt**
-------------------

Premi√®rement nous avons besoin de mesurer la performance de notre mod√®le en utilisant une fonction de co√ªt.

La fonction de co√ªt nous indique la marge d'erreur par rapport √† nos donn√©es existantes.

Il en existe plusieurs mais aujourd'hui nous allons utiliser Root Mean Square Error (RMSE), qui permet de mesurer l'√©cart **quadratique** (√©lev√© au carr√©) moyen entre les valeurs pr√©dites et les valeurs observ√©es.

On part donc du set de donn√©es X tel que :

| ‚Ç¨ Taille (ùë•) | K Prix (ùë¶) |
|--------------|:----------:|
|      10      |     50     |
|      15      |     100    |
|      28      |     210    |
|      34      |     150    |
|      43      |     250    |
|      ...     |     ...    |

les **ùë•** sont les variables d'entr√©e.

les **ùë¶** sont les variables de sortie.

**m** est la taille du set de donn√©es.

**(i)** repr√©sente l'index courant de l'√©l√©ment tel que, d'apr√®s notre set de donn√©es : <img src="/assets/2020-09-23-initiation_ML_regression_lineaire/set-donnees-x2.png" alt="SET-DONNEES-x2" width="100">

Le but est de mesurer l'√©cart entre une hypoth√®se **≈∑** et le vrai r√©sultat **y** par rapport a **x**.

Si l'on prend un exemple concret, nous avons dans notre set de donn√©es en <img src="/assets/2020-09-23-initiation_ML_regression_lineaire/x1.png" alt="x1" width="30"> de **10m2** et un prix de **50k**.

Si on utilise notre mod√®le de pr√©diction pour pr√©dire le prix d'un appartement de **10m2**, plus le r√©sultat sera proche de **50**, plus le mod√®le sera performant.

Nous voulons donc que la valeur **h<sub>&theta;</sub>(x) - ùë¶** soit la plus faible possible.

Cependant, nous ne pouvons pas seulement nous appuyer sur une seule valeur. Notre set de donn√©es comporte plusieurs entr√©es, et ce qui est vrai pour le **30m2** n'est pas forc√©ment vrai pour le **37** et notre but est de trouver un mod√®le qui conviendra le mieux aux deux.

Cela s'appelle la **G√©n√©ralisation**.

Nous allons donc utiliser tout notre set de donn√©es et utiliser la notation avec l'index d√©crit plus haut et somme ‚àë.

Voici le calcul du RMSE, que nous allons expliquer:

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/calcul-rmse.png" alt="CALCUL-RMSE" width="250">

√Ä la mani√®re d'une boucle for en programmation, qui ex√©cute plusieurs fois des instructions tant que l'index n'est pas √©gal √† une autre valeur, ici on d√©finit **i** √† **1** et on fait la somme des carr√©s des √©carts correspondants √† l'index courant tant que **i** n'est pas √©gal a **m**.

Un peu plus haut nous avions calcul√© le coefficient de la droite pour faire une pr√©diction d'un appartement de **40m2**.

Reprenons cette valeur de **6,6** pour exemple et voyons quelle est la marge d'erreur sur notre set de donn√©es.

Nous allons donc faire ce m√™me calcul avec nos 5 premi√®res donn√©es du set que nous avons plus haut.

Nous aurons donc nos pr√©dictions avec notre mod√®le actuel, et les vraies valeur du set.

Par exemple pour le **10m2**, **y = 50**, et notre pr√©diction est **66**.

On aura donc:

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/detail-calcul-rmse.png" alt="DETAIL-CALCUL-RMSE" width="800">

Nous pouvons visualiser ce r√©sultat en l'affichant sur un graphique qui le placerait par rapport au coefficient.

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/graphique-err-coef.png" alt="GRAPHIQUE-ERR-COEF" width="800">

Le probl√®me ici, est qu'on ne sait pas si l'erreur est importante ou pas car nous n'avons aucune comparaison.

Nous avons pris le coefficient **6,6** car rappelons le, nous sommes directement partis d'une estimation visuelle sur le graphique, ce qui √©tait plus simple pour comprendre le fonctionnement mais ce qui est loin d'√™tre exact.

Il conviendra donc d'essayer avec plusieurs param√®tres qui peuvent √™tre 5, 6 , 7 ou n'importe quel autre nombre.

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/graphique-err-coef-comp.png" alt="GRAPHIQUE-ERR-COEF-COMP" width="800">

La valeur la plus exacte est donc celle avec l'erreur la plus basse.

Apr√®s calcul, le meilleur param√®tre pour notre pr√©diction d'appartement est donc en r√©alit√© de **6**.

Si nous reprenons le calcul de notre pr√©diction avec notre nouveau param√®tre, un appartement de **40m2** devrait co√ªter **240K** **‚Ç¨ :**

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/calcul-prediction-new-coeff.png" alt="CALCUL-PREDICTION-NEW-COEF" width="250">

La fonction de co√ªt est ce qu'on appelle une fonction convexe. Trouver le minimum de la courbe, c'est trouver le meilleur param√®tre.

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/graphique-convexe.png" alt="GRAPHIQUE-CONVEXE" width="800">

Ici nous avons fait plusieurs essais en calculant manuellement, mais il existe des outils qui nous permettent de trouver cette valeur.

On en vient donc √† la derni√®re partie de cet article, la descente de gradient.

**Descente de Gradient**
-------------------

La descente de gradient est un algorithme d'optimisation.

C'est un processus it√©ratif, dont le but est de trouver le minimum d'une
fonction convexe.

Le but de la descente de gradient est de minimiser la fonction de co√ªt,
et donc d'avoir le meilleur mod√®le.

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/graphique-target-minimum.png" alt="GRAPHIQUE-TARGET-MINIMUM" width="600">

Plus haut nous avons essay√© avec plusieurs valeurs qui √©taient 5, 6 , 7.

Nous avons donc fait des sauts de 1.

Nous aurions pu faire des sauts plus petits de 0,5 et calculer avec 5,
5.5, 6, 6.5 etc

En r√©alit√©, nous allons utiliser un autre param√®tre qui d√©finit la
taille du pas effectu√© √† chaque it√©ration : le Learning Rate, il se note
‚ç∂.

C'est ce qu'on appelle un hyper-param√®tre, car il s'applique au mod√®le
lui m√™me.

Et pour trouver le bon Learning rate, il conviendra d'essayer plusieurs
fois avec des valeurs diff√©rentes et d'y aller √† t√¢ton.

Pour que l'algorithme de la descente de gradient trouve le minimum de la
fonction convexe, il va devoir calculer la pente de cette fonction. Et
pour calculer la pente d'une fonction, on calcule sa d√©riv√©e.

Nous ne rentrerons pas en profondeur dans ces calculs, mais pour
information, voici la formule du gradient :

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/calcul-gradient.png" alt="CALCUL-GRADIENT" width="350">

Dans notre cas, le param√®tre **a** est notre coefficient, donc le gradient de notre prochain coeff est √©gal au : coeff actuel moins le learning rate multipli√© par la d√©riv√©e de la fonction. Ce calcul est fait pour trouver la convergence de la courbe. Donc, et c'est ce qu'il faut retenir, nous nous dirigeons toujours vers le point le plus bas peu importe o√π nous sommes.

Attention donc √† utiliser un bon learning rate. En effet, s'il est trop grand, alors il risque de ne jamais passer par la valeur minimum. S'il est trop petit, il prendra trop longtemps √† l'atteindre.

![]({{ site.baseurl }}/assets/2020-09-23-initiation_ML_regression_lineaire/learning-rate.png" alt="LEARNING-RATE" width="800">

M√™me si nous ne sommes pas rentr√©s dans tous les d√©tails, nous avons
d√©j√† un bon aper√ßu de comment fonctionne un mod√®le machine Learning.

√Ä noter que nous avons pris un exemple simple avec un seul param√®tre,
mais tout cela peut vite devenir encore plus complexe.

Heureusement pour nous, on ne calcule jamais tout √ßa par nous m√™me, nous avons tous les outils n√©cessaires en programmation comme par exemple <a href="https://scikit-learn.org/stable/">Scikit-Learn</a> ou <a href="https://www.tensorflow.org/?hl=fr">TensorFlow</a> en Python

Il existe encore plein d'autres parties du machine learning √† d√©couvrir comme la classification par exemple, ou bien des mod√®les d'apprentissage non-supervis√©s comme la clusterisation ou la r√©duction dimensionnelle. Ce qui pourra faire l'objet d'autres articles :)

<br/>

Credits :

Intro to data science - Steve Brunton - Washington University

Machine Learning - Andrew Ng - Stanford University

Hands on Machine Learning - Aur√©lien Gu√©ron
