---
contentType: article
lang: fr
date: '2024-07-23'
slug: tester-spark-pytest
title: Tester son script Apache Spark avec pytest
excerpt: >-
  Dans le domaine de la data, la qualit√© de la donn√©es est reine. Il est n√©cessaire de s'en assurer. Pour poser de bonne fondation,
  il est int√©ressant de tester unitairement l'algorithme du traitement Spark. D√©couvrons comment les r√©alisers.
categories: [architecture]
authors:
  - tthuon
cover:
  alt: Un enfant astronaute qui fait une tour en lego
  path: /imgs/articles/2024-07-23-tester-spark-avec-pytest/cover.jpg
keywords: 
- apache spark
- pytest
- data
---

Dans le pr√©c√©dent article sur [d√©marrer avec Apache Spark](/fr/demarrer-apache-spark), nous avons cr√©√© notre premier script de traitement de la donn√©e avec Apache Spark.

Pour s'assurer de la bonne impl√©mentation, nous allons effectuer des tests unitaires.

## Installation de pytest

Dans notre dossier de projet, on va installer [pytest](https://docs.pytest.org/).

<div  class="admonition note"  markdown="1"><p  class="admonition-title">Note</p>
N'oubliez pas d'activer votre environnement virtuel Python avec `. venv/bin/activate`
</div>

```shell
(venv) [thierry@travail:~/eleven/data]
% pip install pytest
```

Avant d'√©crire notre premier test, nous allons r√©organiser notre code pour le rendre testable. En l'√©tat, il est difficile de le tester.

## R√©organisation du code

Pour rappel, voici le code initial.

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.types import IntegerType, DateType

spark = SparkSession.builder.appName("Bike calculation").getOrCreate()

source_file = "source/244400404_comptages-velo-nantes-metropole.csv"
df = spark.read.format("csv").option("delimiter", ";").option("header", True).load(source_file)

df_clean = (
    df.select(
        col("Num√©ro de boucle").alias("loop_number"),
        col("Libell√©").alias("label"),
        col("Total").cast(IntegerType()).alias("total"),
        col("Date format√©e").cast(DateType()).alias("date"),
        col("Vacances").alias("holiday_name"),
    )
    .where(col("Probabilit√© de pr√©sence d'anomalies").isNull())
)

df_clean.write.format("parquet").partitionBy("date").save("datalake/count-bike-nantes.parquet")
```

Nous allons encapsuler le code qui effectue la transformation dans une fonction que nous allons tester.

```python
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col
from pyspark.sql.types import IntegerType, DateType

spark = SparkSession.builder.appName("Bike calculation").getOrCreate()

source_file = "source/244400404_comptages-velo-nantes-metropole.csv"
df = spark.read.format("csv").option("delimiter", ";").option("header", True).load(source_file)

def transformation(spark: SparkSession, df: DataFrame) -> DataFrame:
  return (
      df.select(
          col("Num√©ro de boucle").alias("loop_number"),
          col("Libell√©").alias("label"),
          col("Total").cast(IntegerType()).alias("total"),
          col("Date format√©e").cast(DateType()).alias("date"),
          col("Vacances").alias("holiday_name"),
      )
      .where(col("Probabilit√© de pr√©sence d'anomalies").isNull())
  )

transformation(spark, df).write.format("parquet").partitionBy("date").save("datalake/count-bike-nantes.parquet")
```

Notre code est pr√™t. Pr√©parons les tests.

## √âcriture du test avec pytest

Notre code est d√©pendant de Spark. Il est possible de bouchonner cette d√©pendance, mais c'est une op√©ration assez complexe. 
Le plus simple, selon la [documentation Spark](https://spark.apache.org/docs/latest/api/python/getting_started/testing_pyspark.html#Option-3:-Using-Pytest), est de cr√©er une session Spark d√©di√©e.

Initialisons une_fixture_ avec la session Spark. Elle sera cr√©√©e, partag√©e et d√©truite automatiquement par pytest.

```python
import pytest
from pyspark.sql import SparkSession

@pytest.fixture
def spark_fixture() -> SparkSession:
    spark = SparkSession.builder.appName("Testing Bike calculation").getOrCreate()
    yield spark
```

Nous allons √©galement cr√©er un jeu de donn√©es pour notre test. Dans ce Dataframe de test, je vais mettre une ligne avec la colonne "Probabilit√© de pr√©sence d'anomalies" avec une string vide, et une ligne avec une valeur. Cela va nous permettre de tester la condition `where()`.

```python
import datetime

import pytest
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    DateType,
    IntegerType,
)
from pyspark.testing import assertSchemaEqual, assertDataFrameEqual

from main import transformation

@pytest.fixture
def source_fixture(spark_fixture) -> DataFrame:
    return spark_fixture.createDataFrame(
        [
            (
                "0674",
                "Pont Haudaudine vers Sud",
                "657",
                "",
                "2",
                "0674 - Pont Haudaudine vers Sud",
                "2021-03-16",
                "Hors Vacances",
            ),
            (
                "0676",
                "Pont Willy Brandt vers Beaulieu",
                "480",
                "Faible",
                "1",
                "0676 - Pont Willy Brandt vers Beaulieu",
                "2021-05-31",
                "Hors Vacances",
            ),
        ],
        StructType(
            [
                StructField("Num√©ro de boucle", StringType()),
                StructField("Libell√©", StringType()),
                StructField("Total", StringType()),
                StructField("Probabilit√© de pr√©sence d'anomalies", StringType()),
                StructField("Jour de la semaine", StringType()),
                StructField("Boucle de comptage", StringType()),
                StructField("Date format√©e", StringType()),
                StructField("Vacances", StringType()),
            ]
        ),
    )
```

Pour nos tests, nous allons v√©rifier le sch√©ma du dataframe en sortie de la fonction `transformation()` et v√©rifier les donn√©es. La fonction [assertSchemaEqual()](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.testing.assertSchemaEqual.html) nous facilite le test.

```python
import datetime

import pytest
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    DateType,
    IntegerType,
)
from pyspark.testing import assertSchemaEqual, assertDataFrameEqual

from main import transformation

(...)

def test_schema(spark_fixture: SparkSession, source_fixture: DataFrame):
    df_result = transformation(source_fixture)

    assertSchemaEqual(
        df_result.schema,
        StructType(
            [
                StructField("loop_number", StringType()),
                StructField("label", StringType()),
                StructField("total", IntegerType()),
                StructField("date", DateType()),
                StructField("holiday_name", StringType()),
            ]
        ),
    )
```

Je lance le test.

```shell
% pytest test.pyy -vv
================================= test session starts =================================
platform linux -- Python 3.10.12, pytest-8.0.0, pluggy-1.4.0 -- /usr/bin/python3
cachedir: .pytest_cache
plugins: time-machine-2.13.0, anyio-4.2.0, mock-3.12.0
collected 1 item                                                                      

test.py::test_schema PASSED                                                     [100%]
============================ 1 passed, 2 warnings in 6.44s ============================
```

La structure correspond bien √† l'attendu.

V√©rifions maintenant le contenu du dataframe. La fonction [assertDataFrameEqual()]([https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.testing.assertSchemaEqual.html](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.testing.assertDataFrameEqual.html)) nous facilite le test.

```python
def test_dataframe_content(spark_fixture: SparkSession, source_fixture: DataFrame):
    df_expected = spark_fixture.createDataFrame(
        [
            (
                "0674",
                "Pont Haudaudine vers Sud",
                657,
                datetime.date.fromisoformat("2021-03-16"),
                "Hors Vacances",
            )
        ],
        StructType(
            [
                StructField("loop_number", StringType()),
                StructField("label", StringType()),
                StructField("total", IntegerType()),
                StructField("date", DateType()),
                StructField("holiday_name", StringType()),
            ]
        ),
    )
    df_result = transformation(source_fixture)
    assertDataFrameEqual(df_result, df_expected)
```

Je devrais obtenir qu'une seul ligne, car la seconde ligne dans `source_fixture` la colonne "Probabilit√© de pr√©sence d'anomalies" contient "Faible". Or, je ne veux pas utiliser de donn√©es avec une pr√©sence d'anomalie.

Lan√ßons le test.


```shell
% pytest test.py -vv             
================================= test session starts =================================
platform linux -- Python 3.10.12, pytest-8.0.0, pluggy-1.4.0 -- /usr/bin/python3
cachedir: .pytest_cache
rootdir: /home/thierry/eleven/data
plugins: time-machine-2.13.0, anyio-4.2.0, mock-3.12.0
collected 2 items                                                                     

test.py::test_schema PASSED                                                     [ 50%]
test.py::test_dataframe_content FAILED                                          [100%]

====================================== FAILURES =======================================
_______________________________ test_dataframe_content ________________________________
(...)
E           pyspark.errors.exceptions.base.PySparkAssertionError: [DIFFERENT_ROWS] Results do not match: ( 100.00000 % )
E           *** actual ***
E           ! None
E           
E           
E           *** expected ***
E           ! Row(loop_number='0674', label='Pont Haudaudine vers Sud', total=657, date=datetime.date(2021, 3, 16), holiday_name='Hors Vacances')

../../.local/lib/python3.10/site-packages/pyspark/testing/utils.py:579: PySparkAssertionError
-------------------------------- Captured stderr call ---------------------------------
FAILED test.py::test_dataframe_content - pyspark.errors.exceptions.base.PySparkAssertionError: [DIFFERENT_ROWS] Results do ...
======================= 1 failed, 1 passed, 2 warnings in 7.80s =======================
```

Tiens, c'est bizarre ü§î, le test est en √©chec. Pourtant, le r√©sultat attendu est correct.

Revenons sur le code PySpark, et en particulier sur la condition `where()`.

```python
def transformation(df: DataFrame) -> DataFrame:
    return (
        ...
        .where(col("Probabilit√© de pr√©sence d'anomalies").isNull())
    )
```

En effet, il y a une coquille. D'apr√®s notre jeu de donn√©es, la colonne "Probabilit√© de pr√©sence d'anomalies" est de type string. Or, la fonction `isNull()` est un test sur la nullit√© d'une colonne au sens strict : c'est √©quivalent en SQL √† `IS NULL`. Donc, une colonne de type string vide n'est pas _null_ au sens strict.

Il faut corriger la condition par une √©galit√© avec une string vide.

```python
from pyspark.sql.functions import col, lit

def transformation(df: DataFrame) -> DataFrame:
    return (
        ...
        .where(col("Probabilit√© de pr√©sence d'anomalies") == lit(""))
    )
```

Ainsi, lorsque je relance mon test.

```shell
% pytest test.py -vv                        
================================= test session starts =================================
platform linux -- Python 3.10.12, pytest-8.0.0, pluggy-1.4.0 -- /usr/bin/python3
cachedir: .pytest_cache
plugins: time-machine-2.13.0, anyio-4.2.0, mock-3.12.0
collected 2 items                                                                     

test.py::test_schema PASSED                                                     [ 50%]
test.py::test_dataframe_content PASSED                                          [100%]
============================ 2 passed, 2 warnings in 8.33s ============================
```

F√©licitations, votre code est maintenant test√©. Vous pouvez aller en production sereinement üòå.

## Conclusion

A travers cet article, nous avons vu la mise en place de tests unitaire pour notre traitement de donn√©es PySpark. Cela nous a permis de nous rendre compte qu'il y avait une erreur dans le code. Ainsi, nous avons pu le corriger. Nous savons maintenant que le code produit r√©pond √† nos attentes, ainsi qu'aux utilisateurs de la donn√©e.

## R√©f√©rences

Code complet

```python
# main.py
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col, lit
from pyspark.sql.types import IntegerType, DateType

spark = SparkSession.builder.appName("Bike calculation").getOrCreate()

source_file = "source/244400404_comptages-velo-nantes-metropole.csv"
df = spark.read.format("csv").option("delimiter", ";").option("header", True).load(source_file)


def transformation(df: DataFrame) -> DataFrame:
    return (
        df.select(
            col("Num√©ro de boucle").alias("loop_number"),
            col("Libell√©").alias("label"),
            col("Total").cast(IntegerType()).alias("total"),
            col("Date format√©e").cast(DateType()).alias("date"),
            col("Vacances").alias("holiday_name"),
        )
        .where(col("Probabilit√© de pr√©sence d'anomalies") == lit(""))
    )


transformation(df).write.format("parquet").partitionBy("date").save("datalake/count-bike-nantes.parquet")
```

```python
# test.py
import datetime

import pytest
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    DateType,
    IntegerType,
)
from pyspark.testing import assertSchemaEqual, assertDataFrameEqual

from main import transformation


@pytest.fixture
def spark_fixture() -> SparkSession:
    spark = SparkSession.builder.appName("Testing Bike calculation").getOrCreate()
    yield spark


@pytest.fixture
def source_fixture(spark_fixture) -> DataFrame:
    return spark_fixture.createDataFrame(
        [
            (
                "0674",
                "Pont Haudaudine vers Sud",
                "657",
                "",
                "2",
                "0674 - Pont Haudaudine vers Sud",
                "2021-03-16",
                "Hors Vacances",
            ),
            (
                "0676",
                "Pont Willy Brandt vers Beaulieu",
                "480",
                "Faible",
                "1",
                "0676 - Pont Willy Brandt vers Beaulieu",
                "2021-05-31",
                "Hors Vacances",
            ),
        ],
        StructType(
            [
                StructField("Num√©ro de boucle", StringType()),
                StructField("Libell√©", StringType()),
                StructField("Total", StringType()),
                StructField("Probabilit√© de pr√©sence d'anomalies", StringType()),
                StructField("Jour de la semaine", StringType()),
                StructField("Boucle de comptage", StringType()),
                StructField("Date format√©e", StringType()),
                StructField("Vacances", StringType()),
            ]
        ),
    )


def test_schema(spark_fixture: SparkSession, source_fixture: DataFrame):
    df_result = transformation(source_fixture)

    assertSchemaEqual(
        df_result.schema,
        StructType(
            [
                StructField("loop_number", StringType()),
                StructField("label", StringType()),
                StructField("total", IntegerType()),
                StructField("date", DateType()),
                StructField("holiday_name", StringType()),
            ]
        ),
    )


def test_dataframe_content(spark_fixture: SparkSession, source_fixture: DataFrame):
    df_expected = spark_fixture.createDataFrame(
        [
            (
                "0674",
                "Pont Haudaudine vers Sud",
                657,
                datetime.date.fromisoformat("2021-03-16"),
                "Hors Vacances",
            )
        ],
        StructType(
            [
                StructField("loop_number", StringType()),
                StructField("label", StringType()),
                StructField("total", IntegerType()),
                StructField("date", DateType()),
                StructField("holiday_name", StringType()),
            ]
        ),
    )

    df_result = transformation(source_fixture)
    assertDataFrameEqual(df_result, df_expected)
```

```text
# requirements.txt
pyspark==3.5.0
pytest
```
