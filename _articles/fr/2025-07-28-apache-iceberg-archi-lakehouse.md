---
contentType: article
lang: fr
date: 2025-07-28
slug: apache-iceberg-archi-lakehouse
title: Apache Iceberg pour une architecture lakehouse sur AWS
excerpt: Ce guide prÃ©sente Apache Iceberg, un format de table moderne pour les donnÃ©es volumineuses, la gestion des versions et des performances optimisÃ©es.
categories:
  - architecture
keywords:
  - data
  - iceberg
  - lakehouse
  - big data
  - aws
authors:
  - ahoudaibi
cover:
  alt: Apache Iceberg pour une architecture lakehouse sur AWS
  path: /imgs/articles/2025-07-28-apache-iceberg-archi-lakehouse/cover.png
seo:
  title: "Architecture lakehouse avec Apache Iceberg sur AWS"
  description: DÃ©couvrez comment utiliser Apache Iceberg sur AWS pour crÃ©er une architecture adaptÃ©e aux enjeux de gouvernance, scalabilitÃ© et requÃªtage optimisÃ©.
---

Apache Iceberg est un format de table ouvert pour les donnÃ©es volumineuses, conÃ§u pour rÃ©soudre les problÃ¨mes de performance et de fiabilitÃ© des formats traditionnels comme Apache Hive. Il offre une approche moderne pour gÃ©rer les mÃ©tadonnÃ©es de tables, permettant des opÃ©rations ACID, la gestion des versions et des performances optimisÃ©es pour les requÃªtes analytiques.

## Architecture et fonctionnement d'Apache Iceberg

### SchÃ©ma d'architecture

![alt of image]({BASE_URL}/imgs/articles/2025-07-28-apache-iceberg-archi-lakehouse/iceberg-archi.png)

### Structure des tables Iceberg
Apache Iceberg utilise une architecture hiÃ©rarchique de mÃ©tadonnÃ©es composÃ©e de trois niveaux principaux pour gÃ©rer efficacement les donnÃ©es :

#### Catalog
Point d'entrÃ©e qui maintient la liste des tables et leurs emplacements. Il peut Ãªtre implÃ©mentÃ© via diffÃ©rents backends comme AWS Glue, Apache Hive Metastore, ou des bases de donnÃ©es relationnelles.

#### Metadata
- **Tables Metadata** :
  Contient les informations sur le schÃ©ma de la table, le partitionnement, les snapshots et l'historique des modifications. Chaque version de la table est reprÃ©sentÃ©e par un fichier de mÃ©tadonnÃ©es unique au format JSON. Ces mÃ©tadonnÃ©es incluent :
  - Le schÃ©ma des colonnes des fichiers
  - Les stratÃ©gies de partitionnement appliquÃ©es aux donnÃ©es
  - L'historique des snapshots pointant vers diffÃ©rentes versions des fichiers
- **Manifest Files** :
  Fichiers qui contiennent la liste des fichiers de donnÃ©es et leurs statistiques dÃ©taillÃ©es (min/max, nombre de lignes, taille des fichiers, etc.). Ils permettent l'optimisation des requÃªtes grÃ¢ce au pruning efficace des fichiers qui ne correspondent pas aux critÃ¨res de filtrage.
- **Manifest Lists** :
  Fichiers qui contiennent la liste des manifest files associÃ©s Ã  un snapshot donnÃ©. Ils permettent de regrouper et d'organiser les manifest files par partition ou par opÃ©ration, facilitant ainsi la navigation dans la structure des mÃ©tadonnÃ©es.

#### Data Files
Les donnÃ©es rÃ©elles sont stockÃ©es dans des fichiers **Parquet**,  **ORC**, ou **Avro**, organisÃ©s selon la structure de partitionnement dÃ©finie dans les mÃ©tadonnÃ©es. Ces fichiers contiennent :
* Les donnÃ©es tabulaires au format colonnaire optimisÃ©
* Les mÃ©tadonnÃ©es intÃ©grÃ©es (schÃ©ma, statistiques par colonne)
* La compression et l'encodage adaptÃ©s aux types de donnÃ©es
* L'organisation physique des donnÃ©es selon les partitions dÃ©finies

Les fichiers constituent la couche de stockage effective des donnÃ©es, tandis que les manifest files et table metadata permettent de les localiser et de les interroger efficacement. Dans la suite, nous choisirons le format Parquet comme type de fichiers. En effet il s'agit du format par dÃ©faut lors de l'utilisation de Iceberg. NÃ©anmoins le format de fichier contenant les donnÃ©es est modifiable via ce paramÃ¨tre : *write.format.default*.


## Avantages par rapport Ã  des fichiers Parquet classiques

Cette architecture mÃ©tadonnÃ©es-centrÃ©e d'Iceberg apporte plusieurs bÃ©nÃ©fices spÃ©cifiques aux fichiers Parquet :

- **Ã‰volution de schÃ©ma** : Modification du schÃ©ma des colonnes Parquet sans rÃ©Ã©criture des fichiers existants
- **Partitionnement intelligent** : Gestion automatique du partitionnement des fichiers Parquet selon les besoins
- **Optimisation des requÃªtes** : Exploitation des statistiques Parquet au niveau des manifest files pour un pruning ultra-efficace
- **Versioning des donnÃ©es** : Suivi des modifications des fichiers Parquet avec possibilitÃ© de rollback

### Gestion des versions et snapshots

Iceberg maintient un historique complet des modifications via un systÃ¨me de snapshots. Chaque modification (INSERT, UPDATE, DELETE) crÃ©e un nouveau snapshot sans affecter les prÃ©cÃ©dents, permettant ainsi le time travel et la lecture cohÃ©rente des donnÃ©es.

### Optimisations de performance

Le format intÃ¨gre plusieurs mÃ©canismes d'optimisation :

- **Schema Evolution** : Modification du schÃ©ma sans rÃ©Ã©criture des donnÃ©es existantes
- **Partition Evolution** : Changement de stratÃ©gie de partitionnement transparent
- **Compaction** : Optimisation automatique des fichiers de donnÃ©es
- **Predicate Pushdown** : Filtrage efficace au niveau des mÃ©tadonnÃ©es

AprÃ¨s avoir explorÃ© les fondements et avantages d'Apache Iceberg, il est pertinent de se pencher sur son dÃ©ploiement concret dans un environnement cloud.

## IntÃ©gration sur AWS
AWS propose une suite de services compatibles avec Apache Iceberg, facilitant son intÃ©gration Ã  grande Ã©chelle dans des architectures data modernes. Voici comment Iceberg peut Ãªtre mis en Å“uvre efficacement dans cet Ã©cosystÃ¨me.

[AWS Glue Data Catalog](https://docs.aws.amazon.com/fr_fr/glue/latest/dg/catalog-and-crawler.html) sert de catalog Iceberg natif, offrant une intÃ©gration transparente avec l'Ã©cosystÃ¨me AWS. Il gÃ¨re automatiquement les mÃ©tadonnÃ©es et assure la compatibilitÃ© avec les services AWS comme Amazon Athena, Amazon EMR et AWS Glue ETL.

[AWS S3](https://aws.amazon.com/fr/s3/) constitue le backend de stockage idÃ©al pour Iceberg sur AWS, offrant :
- **DurabilitÃ©** : 99,999999999% de durabilitÃ© des donnÃ©es
- **ScalabilitÃ©** : Stockage illimitÃ© avec performances constantes
- **IntÃ©gration** : CompatibilitÃ© native avec tous les services AWS
- **SÃ©curitÃ©** : Chiffrement au repos et en transit

[AWS S3 Tables](https://aws.amazon.com/fr/s3/features/tables/) une nouvelle classe de stockage managÃ©e pour Iceberg. Les tables Amazon S3 sont jusquâ€™Ã  3 fois plus rapides par rapport aux tables Iceberg non gÃ©rÃ©es, et peuvent supporter jusquâ€™Ã  10 fois plus de transactions par seconde par rapport aux tables Iceberg stockÃ©es dans des compartiments S3 Ã  usage gÃ©nÃ©ral.
- **Automatisation** des mÃ©tadonnÃ©es, compaction et maintenance
- **RÃ©duction des coÃ»ts** opÃ©rationnels


CÃ´tÃ© services, plusieurs outils AWS viennent enrichir lâ€™exploitation des tables Iceberg : 
 - **Amazon Athena** : RequÃªtes SQL directes sur les tables Iceberg sans infrastructure Ã  gÃ©rer. Support natif du time travel et des opÃ©rations ACID.
 - **Amazon EMR** : Clusters pour le traitement des donnÃ©es Iceberg avec Apache Spark, permettant des workloads de transformation complexes.
 - **AWS Glue ETL** : Jobs ETL serverless avec support natif d'Iceberg pour les pipelines de donnÃ©es.
 - **Amazon Redshift** : IntÃ©gration via Redshift Spectrum pour l'analyse des donnÃ©es Iceberg depuis l'entrepÃ´t de donnÃ©es.
 - **Amazon Data Firehose** : Service de streaming entiÃ¨rement gÃ©rÃ© permettant l'ingestion de donnÃ©es en temps rÃ©el directement dans des tables Apache Iceberg stockÃ©es sur Amazon S3.

Maintenant que les concepts clÃ©s et lâ€™intÃ©gration dâ€™Iceberg dans un environnement cloud comme AWS ont Ã©tÃ© abordÃ©s, voyons comment cela se traduit concrÃ¨tement sur le terrain.


## Apache Iceberg en pratique

Pour mieux comprendre ce format de table, nous allons utiliser Spark (PySpark) afin de manipuler des donnÃ©es et explorer ses capacitÃ©s.

### Configuration de l'environnement local

Reprenons le code de notre prÃ©cÃ©dent article rÃ©digÃ© par Thierry T. [DÃ©marrer avec Apache Spark Ã©tape par Ã©tape](/fr/demarrer-apache-spark/).  
Ici afin d'explorer les diffÃ©rentes fonctionnalitÃ©s du format Iceberg, nous utiliserons la configuration suivante: 

**Catalog** :
 Base de donnÃ©es relationnelle en JDBC avec *SQLite* dans un fichier nommÃ© : *mydb*

**Stockage** : 
 Un bucket S3 grÃ¢ce Ã  MinIO sur Docker nous servira d'espace de stockage. GrÃ¢ce au docker file ci dessous et Ã  la commande suivante :  

```bash 
 docker compose up -d
 ```
 Nous allons crÃ©e un bucket S3 nommÃ© : *local-bucket*.
 On pourra s'y connecter via l'UI en se rendant sur notre navigateur Ã  l'url suivante : *http://localhost:9001/* et ainsi s'identifier avec le user et password suivant : *MyUserTest1*

```docker
version: "3.9"
name: iceberg-bucket

services:
  minio:
    image: minio/minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: MyUserTest1
      MINIO_ROOT_PASSWORD: MyUserTest1
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data

  createbuckets:
    image: quay.io/minio/mc:RELEASE.2025-03-12T17-29-24Z
    depends_on:
      - minio
    restart: on-failure
    entrypoint: >
      /bin/sh -c "
      sleep 5;
      /usr/bin/mc alias set dockerminio http://minio:9000 MyUserTest1 MyUserTest1;
      /usr/bin/mc mb dockerminio/local-bucket;
      exit 0;
      "

volumes:
  minio_data:
```

**Spark** :
Voici comment la session spark est dÃ©finie (version 3.5.6):

```python
from pyspark.sql import SparkSession

S3_BUCKET_NAME = "local-bucket"
MINIO_ROOT_USER = MINIO_ROOT_PASSWORD = "MyUserTest1"
spark = (
        SparkSession.builder.appName("Iceberg4Lakehouse")
        .config(
            "spark.jars.packages",
            "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.1,org.apache.iceberg:iceberg-aws-bundle:1.4.0,org.xerial:sqlite-jdbc:3.46.0.0",
        )
        .config(
            "spark.sql.extensions",
            "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions",
        )
        .config(
            "spark.sql.catalog.local_catalog", "org.apache.iceberg.spark.SparkCatalog"
        )
        .config(
            "spark.sql.catalog.local_catalog.catalog-impl",
            "org.apache.iceberg.jdbc.JdbcCatalog",
        )
        .config(
            "spark.sql.catalog.local_catalog.io-impl",
            "org.apache.iceberg.aws.s3.S3FileIO",
        )
        .config(
            "spark.sql.catalog.local_catalog.uri",
            "jdbc:sqlite:file:mydb",
        )
        .config(
            "spark.sql.catalog.local_catalog.warehouse",
            f"s3a://{S3_BUCKET_NAME}/DATA",
        )
        .config(
            "spark.sql.catalog.local_catalog.s3.endpoint",
            "http://localhost:9000",
        )
        .config("spark.sql.catalog.local_catalog.s3.access-key-id", MINIO_ROOT_USER)
        .config("spark.sql.catalog.local_catalog.s3.secret-access-key", MINIO_ROOT_PASSWORD)
        .config("spark.sql.catalog.local_catalog.s3.path-style-access", "true")
        .config("spark.sql.catalog.local_catalog.drop-table-include-data", "true")
        .config("spark.sql.iceberg.check-ordering", "false")
        .config("spark.sql.catalog.local_catalog.jdbc.schema-version", "V1")
        .getOrCreate()
    )
```
Maintenant que les bases sont posÃ©es, voyons concrÃ¨tement comment interagir avec une table Iceberg Ã  travers diffÃ©rentes opÃ©rations, telles que lâ€™insertion de donnÃ©es, la mise Ã  jour, ou encore la gestion de lâ€™Ã©volution du schÃ©ma.

## Quelques opÃ©rations de base

### Create
Avant de manipuler des donnÃ©es, il faut dâ€™abord crÃ©er une table Iceberg. Celle-ci peut Ãªtre dÃ©finie Ã  lâ€™aide dâ€™une simple requÃªte SQL, avec le schÃ©ma de colonnes souhaitÃ© et le format de stockage appropriÃ© ou directement Ã  partir d'un dataframe spark. Voici un exemple de crÃ©ation de table avec partitionnement.

```python
    df = spark.createDataFrame(
        [
            (1, "Alice", "2023-01-01"),
            (2, "Bob", "2023-01-02"),
            (3, "Anna", "2023-01-02"),
            (4, "Tom", "2023-01-03"),
        ],
        ["id", "name", "signup_date"],
    )
    spark.sql("use local_catalog")
    spark.sql("CREATE DATABASE IF NOT EXISTS db")
    df.writeTo("db.signup").partitionedBy("signup_date").createOrReplace()
    spark.read.format("iceberg").load("db.signup").orderBy("id").show()
```
```text
+---+-----+-----------+
| id| name|signup_date|
+---+-----+-----------+
|  1|Alice| 2023-01-01|
|  2|  Bob| 2023-01-02|
|  3| Anna| 2023-01-02|
|  4|  Tom| 2023-01-03|
+---+-----+-----------+
```

En se connectant Ã  la console *Minio* au chemin suivant: *local-bucket/DATA/db/signup/data*. On voit bien dans l'espace de stockage que les donnÃ©es sont physiquement partitionnÃ©s par date.

![alt of image]({BASE_URL}/imgs/articles/2025-07-28-apache-iceberg-archi-lakehouse/physical_partition.png)

### Insert
Une fois la table crÃ©Ã©e, on peut y insÃ©rer des donnÃ©es. Apache Iceberg supporte lâ€™insertion, ce qui permet dâ€™ajouter de nouvelles lignes de maniÃ¨re simple et transactionnelle, mÃªme dans un environnement distribuÃ©.

```python
    df_new = spark.createDataFrame(
        [
            (5, "Sarah", "2023-01-04"),
            (6, "Mike", "2023-01-04"),
            (7, "Tony", "2023-01-04"),
        ],
        ["id", "name", "signup_date"],
    )
    # Insertion simple avec append
    df_new.writeTo("db.signup").append()
    spark.read.format("iceberg").load("db.signup").orderBy("id").show()
```
```text
+---+-----+-----------+
| id| name|signup_date|
+---+-----+-----------+
|  1|Alice| 2023-01-01|
|  2|  Bob| 2023-01-02|
|  3| Anna| 2023-01-02|
|  4|  Tom| 2023-01-03|
|  5|Sarah| 2023-01-04|
|  6| Mike| 2023-01-04|
|  7| Tony| 2023-01-04|
+---+-----+-----------+
```

### Merge (ici stratÃ©gie en UPSERT)
Apache Iceberg prend Ã©galement en charge lâ€™instruction `MERGE INTO`, qui permet de rÃ©aliser des opÃ©rations de type *upsert*.  
Un **upsert** est une combinaison de deux opÃ©rations : **update** (mise Ã  jour) et **insert** (insertion). Si la donnÃ©e existe dÃ©jÃ  (selon une condition), elle est mise Ã  jour ; sinon, elle est insÃ©rÃ©e.  
Câ€™est une opÃ©ration particuliÃ¨rement utile pour intÃ©grer des flux de donnÃ©es incrÃ©mentales ou faire de la synchronisation avec des sources externes.

```python
   # Cette stratÃ©gie combine mise Ã  jour et insertion :
    # DonnÃ©es mixtes (mises Ã  jour + nouveaux enregistrements)
    df_mixed = spark.createDataFrame(
        [
            (1, "Alice Johnson", "2023-01-01"),  # Mise Ã  jour
            (7, "Emma", "2023-01-05"),  # Nouveau
            (8, "David", "2023-01-05"),  # Nouveau
        ],
        ["id", "name", "signup_date"],
    ).createOrReplaceTempView("df_mixed")

    spark.sql(
        """
        MERGE INTO db.signup AS target
        USING df_mixed AS source
        ON target.id = source.id
        WHEN MATCHED THEN UPDATE SET *
        WHEN NOT MATCHED THEN INSERT * """
    )
    spark.read.format("iceberg").load("db.signup").orderBy("id").show()
```
```text
+---+-------------+-----------+
| id|         name|signup_date|
+---+-------------+-----------+
|  1|Alice Johnson| 2023-01-01|
|  2|          Bob| 2023-01-02|
|  3|         Anna| 2023-01-02|
|  4|          Tom| 2023-01-03|
|  5|        Sarah| 2023-01-04|
|  6|         Mike| 2023-01-04|
|  7|         Emma| 2023-01-05|
|  8|        David| 2023-01-05|
+---+-------------+-----------+
```

### Insert avec Ã©volution du schema automatique
Lâ€™un des avantages majeurs dâ€™Apache Iceberg est sa capacitÃ© Ã  gÃ©rer lâ€™Ã©volution de schÃ©ma sans interruption. Cela signifie quâ€™il est possible dâ€™insÃ©rer des donnÃ©es contenant de nouvelles colonnes non encore prÃ©sentes dans la table, sans gÃ©nÃ©rer dâ€™erreur. Iceberg ajoutera automatiquement les colonnes manquantes au schÃ©ma, tout en conservant les versions prÃ©cÃ©dentes pour garantir la compatibilitÃ© et le time travel.

```python
    spark.sql(
        f"ALTER TABLE db.signup SET TBLPROPERTIES ('write.spark.accept-any-schema'='true')"
    )
    df_with_email = spark.createDataFrame(
        [
            (9, "Paul", "2023-01-09", "paul@example.com"),
            (10, "Marie", "2023-01-09", "marie@example.com"),
        ],
        ["id", "name", "signup_date", "email"],
    )
    # Iceberg supporte l'Ã©volution du schÃ©ma automatiquement
    df_with_email.writeTo("db.signup").option("mergeSchema","true").append()
    spark.read.format("iceberg").load("db.signup").orderBy("id").show()
```
```text
+---+-------------+-----------+-----------------+
| id|         name|signup_date|            email|
+---+-------------+-----------+-----------------+
|  1|Alice Johnson| 2023-01-01|             NULL|
|  2|          Bob| 2023-01-02|             NULL|
|  3|         Anna| 2023-01-02|             NULL|
|  4|          Tom| 2023-01-03|             NULL|
|  5|        Sarah| 2023-01-04|             NULL|
|  6|         Mike| 2023-01-04|             NULL|
|  7|         Emma| 2023-01-05|             NULL|
|  8|        David| 2023-01-05|             NULL|
|  9|         Paul| 2023-01-09| paul@example.com|
| 10|        Marie| 2023-01-09|marie@example.com|
+---+-------------+-----------+-----------------+
```
ðŸ’¡NB : Il est Ã©galement possible de gÃ©rer lâ€™Ã©volution du schÃ©ma manuellement en ajoutant explicitement les colonnes via une commande `ALTER TABLE`, avant lâ€™insertion des donnÃ©es :
```python
    df_with_last_name = spark.createDataFrame(
        [
            (9, "Paul", "2023-01-09", "paul@example.com", "Derrick"),
            (10, "Marie", "2023-01-09", "marie@example.com", "Donovan"),
        ],
        ["id", "name", "signup_date", "email", "last_name"],
    )
    spark.sql(""" ALTER TABLE db.signup ADD COLUMN last_name STRING """)
    df_with_last_name.writeTo("db.signup").append()
    spark.read.format("iceberg").load("db.signup").orderBy("id").show()
```
```text
---------+--------+-----------+-----------------+---------+
| id|         name|signup_date|            email|last_name|
+---+-------------+-----------+-----------------+---------+
|  1|Alice Johnson| 2023-01-01|             NULL|     NULL|
|  2|          Bob| 2023-01-02|             NULL|     NULL|
|  3|         Anna| 2023-01-02|             NULL|     NULL|
|  4|          Tom| 2023-01-03|             NULL|     NULL|
|  5|        Sarah| 2023-01-04|             NULL|     NULL|
|  6|         Mike| 2023-01-04|             NULL|     NULL|
|  7|         Emma| 2023-01-05|             NULL|     NULL|
|  8|        David| 2023-01-05|             NULL|     NULL|
|  9|         Paul| 2023-01-09| paul@example.com|     NULL|
| 10|        Marie| 2023-01-09|marie@example.com|     NULL|
| 11|         Jack| 2023-01-09| jack@example.com|  Derrick|
| 12|         Lola| 2023-01-09| lola@example.com|  Donovan|
+---+-------------+-----------+-----------------+---------+
```

### Listing des snapshots
```python
    snapshots_df = spark.sql(f"SELECT * FROM db.signup.snapshots").orderBy(
        "committed_at",
    )
    snapshots_df.show()
```

```text
+-------------+-------------------+-------------------+---------+--------------------+--------------------+
| committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|
+-------------+-------------------+-------------------+---------+--------------------+--------------------+
|2025-...     |7759085558406892563|               NULL|overwrite|s3a://local-bucke...|{spark.app.id -> ...|
|2025-...     |8755520537945265641|7759085558406892563|   append|s3a://local-bucke...|{spark.app.id -> ...|
|2025-...     |3147838452100064630|8755520537945265641|overwrite|s3a://local-bucke...|{spark.app.id -> ...|
|2025-...     |1336414532923420140|3147838452100064630|overwrite|s3a://local-bucke...|{spark.app.id -> ...|
|2025-...     |9100672136501671111|1336414532923420140|   append|s3a://local-bucke...|{spark.app.id -> ...|
|2025-...     |4524301540483360157|9100672136501671111|   append|s3a://local-bucke...|{spark.app.id -> ...|
+-------------+-------------------+-------------------+---------+--------------------+--------------------+
```

### Lecture Ã  partir d'un snapshot prÃ©cÃ©dent (time travel)
```python
    first_snapshot_id = snapshots_df.limit(1).collect()[0].snapshot_id
    spark.read.option("snapshot-id", first_snapshot_id).format("iceberg").load(
        "db.signup"
    ).orderBy("id").show()
```
Nous voici Ã  la premiÃ¨re version de la table !
```text
+---+-----+-----------+
| id| name|signup_date|
+---+-----+-----------+
|  1|Alice| 2023-01-01|
|  2|  Bob| 2023-01-02|
|  3| Anna| 2023-01-02|
|  4|  Tom| 2023-01-03|
+---+-----+-----------+
```
AprÃ¨s avoir explorÃ© les fonctionnalitÃ©s clÃ©s et les intÃ©grations possibles avec Apache Iceberg, il est essentiel dâ€™adopter certaines bonnes pratiques pour garantir la performance, la fiabilitÃ© et la maintenabilitÃ© de vos pipelines de donnÃ©es.


## Bonnes pratiques
Cette section propose quelques pistes et recommandations pour mieux tirer parti dâ€™Iceberg au quotidien, notamment autour de la gestion des fichiers, lâ€™Ã©volution des schÃ©mas, la surveillance et la sÃ©curitÃ©. Ces optimisations peuvent contribuer Ã  faciliter le fonctionnement fluide et Ã©volutif de votre architecture data.

- **Partitionnement optimal**  
    Choisir une stratÃ©gie de partitionnement adaptÃ©e aux patterns de requÃªtes les plus frÃ©quents. Ã‰viter le sur-partitionnement qui peut dÃ©grader les performances.

- **Gestion des petits fichiers**  
    Utiliser rÃ©guliÃ¨rement les opÃ©rations de compaction pour Ã©viter l'accumulation de petits fichiers qui dÃ©gradent les performances de lecture. La commande suivante lance la compaction des fichiers de donnÃ©es de la table signup, regroupant les petits fichiers en fichiers plus volumineux et plus efficaces Ã  lire.
    ```sql
    CALL local_catalog.system.rewrite_data_files(table => 'signup')
    ```
- **Maintenance des tables Iceberg**  
    Effectuer rÃ©guliÃ¨rement des opÃ©rations de maintenance permet de prÃ©server la performance et la fiabilitÃ© des tables Iceberg dans le temps. Cela inclut notamment:

    La suppression des snapshots obsolÃ¨tes Ã  lâ€™aide de la commande *expire_snapshots*, pour Ã©viter une accumulation inutile de mÃ©tadonnÃ©es.

    La purge des fichiers orphelins via *remove_orphan_files*, afin de libÃ©rer de lâ€™espace et de maintenir la cohÃ©rence du stockage.

    La gestion proactive des mÃ©tadonnÃ©es avec *rewrite_manifests*, qui permet dâ€™optimiser la structure des fichiers de manifestes, en particulier aprÃ¨s de nombreuses insertions ou suppressions.

- **SÃ©curitÃ© et gouvernance**  
    ImplÃ©menter des politiques granulaires (AWS IAM) pour contrÃ´ler l'accÃ¨s aux donnÃ©es et une gouvernance avancÃ©e du data lake (grÃ¢ce Ã  AWS Lake Formation).

- **Monitoring**  
    Surveiller les mÃ©triques de performance pour le tracing des requÃªtes complexes. Par exemple, Athena peut publier automatiquement les mÃ©triques des requÃªtes dans CloudWatch, ce qui permet :  
    - De crÃ©er des **alertes personnalisÃ©es** (ex. : alerte si une requÃªte dÃ©passe un certain temps dâ€™exÃ©cution).  
    - De visualiser lâ€™Ã©volution des performances dans le temps via des dashboards CloudWatch.  
    - Dâ€™analyser les logs dâ€™erreurs pour dÃ©tecter les anomalies.


## Conclusion

Apache Iceberg offre une solution robuste pour la gestion des donnÃ©es volumineuses avec des capacitÃ©s ACID complÃ¨tes, une Ã©volution de schÃ©ma flexible et des performances optimisÃ©es. Son intÃ©gration native avec l'Ã©cosystÃ¨me AWS en fait un choix idÃ©al pour les architectures de donnÃ©es modernes de type LakeHouse nÃ©cessitant fiabilitÃ©, scalabilitÃ© et performance.

L'adoption d'Iceberg permet de surmonter les limitations des formats traditionnels tout en bÃ©nÃ©ficiant de la puissance et de la flexibilitÃ© des services AWS managÃ©s.


## RÃ©fÃ©rences

- https://aws.amazon.com/fr/what-is/apache-iceberg/
- https://iceberg.apache.org/
- https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg.html
- https://docs.aws.amazon.com/redshift/latest/dg/querying-iceberg.html
- https://iceberg.apache.org/docs/nightly/spark-getting-started/#using-iceberg-in-spark-3
- https://aws.amazon.com/fr/s3/features/tables/

Code complet

```python
# main.py
from pyspark.sql import SparkSession

S3_BUCKET_NAME = "local-bucket"
MINIO_ROOT_USER = MINIO_ROOT_PASSWORD = "MyUserTest1"


def main():
    spark = (
        SparkSession.builder.appName("Iceberg4Lakehouse")
        .config(
            "spark.jars.packages",
            "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.1,org.apache.iceberg:iceberg-aws-bundle:1.4.0,org.xerial:sqlite-jdbc:3.46.0.0",
        )
        .config(
            "spark.sql.extensions",
            "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions",
        )
        .config(
            "spark.sql.catalog.local_catalog", "org.apache.iceberg.spark.SparkCatalog"
        )
        .config(
            "spark.sql.catalog.local_catalog.catalog-impl",
            "org.apache.iceberg.jdbc.JdbcCatalog",
        )
        .config(
            "spark.sql.catalog.local_catalog.io-impl",
            "org.apache.iceberg.aws.s3.S3FileIO",
        )
        .config(
            "spark.sql.catalog.local_catalog.uri",
            "jdbc:sqlite:file:mydb",
        )
        .config(
            "spark.sql.catalog.local_catalog.warehouse",
            f"s3a://{S3_BUCKET_NAME}/DATA",
        )
        .config(
            "spark.sql.catalog.local_catalog.s3.endpoint",
            "http://localhost:9000",
        )
        .config("spark.sql.catalog.local_catalog.s3.access-key-id", MINIO_ROOT_USER)
        .config(
            "spark.sql.catalog.local_catalog.s3.secret-access-key", MINIO_ROOT_PASSWORD
        )
        .config("spark.sql.catalog.local_catalog.s3.path-style-access", "true")
        .config("spark.sql.catalog.local_catalog.drop-table-include-data", "true")
        .config("spark.sql.iceberg.check-ordering", "false")
        .config("spark.sql.catalog.local_catalog.jdbc.schema-version", "V1")
        .getOrCreate()
    )

    # 1. Create
    df = spark.createDataFrame(
        [
            (1, "Alice", "2023-01-01"),
            (2, "Bob", "2023-01-02"),
            (3, "Anna", "2023-01-02"),
            (4, "Tom", "2023-01-03"),
        ],
        ["id", "name", "signup_date"],
    )
    spark.sql("use local_catalog")
    spark.sql("CREATE DATABASE IF NOT EXISTS db")
    df.writeTo("db.signup").partitionedBy("signup_date").createOrReplace()
    spark.read.format("iceberg").load("db.signup").orderBy("id").show()

    # 2. Insert
    df_new = spark.createDataFrame(
        [
            (5, "Sarah", "2023-01-04"),
            (6, "Mike", "2023-01-04"),
            (7, "Tony", "2023-01-04"),
        ],
        ["id", "name", "signup_date"],
    )
    # Insertion simple avec append
    df_new.writeTo("db.signup").append()
    spark.read.format("iceberg").load("db.signup").orderBy("id").show()

    # 3. Merge (UPSERT)
    # Cette stratÃ©gie combine mise Ã  jour et insertion :
    # DonnÃ©es mixtes (mises Ã  jour + nouveaux enregistrements)
    df_mixed = spark.createDataFrame(
        [
            (1, "Alice Johnson", "2023-01-01"),  # Mise Ã  jour
            (7, "Emma", "2023-01-05"),  # Nouveau
            (8, "David", "2023-01-05"),  # Nouveau
        ],
        ["id", "name", "signup_date"],
    ).createOrReplaceTempView("df_mixed")

    spark.sql(
        """
        MERGE INTO db.signup AS target
        USING df_mixed AS source
        ON target.id = source.id
        WHEN MATCHED THEN UPDATE SET *
        WHEN NOT MATCHED THEN INSERT * """
    )
    df_iceberg = spark.read.format("iceberg").load("db.signup")
    df_iceberg.orderBy("id").show()

    # 4. Insert avec Ã©volution du schema automatique
    spark.sql(
        f"ALTER TABLE db.signup SET TBLPROPERTIES ('write.spark.accept-any-schema'='true')"
    )
    df_with_email = spark.createDataFrame(
        [
            (9, "Paul", "2023-01-09", "paul@example.com"),
            (10, "Marie", "2023-01-09", "marie@example.com"),
        ],
        ["id", "name", "signup_date", "email"],
    )
    # Iceberg supporte l'Ã©volution du schÃ©ma automatiquement
    df_with_email.writeTo("db.signup").option("mergeSchema", "true").append()
    df_iceberg = spark.read.format("iceberg").load("db.signup").orderBy("id").show()

    # 5. Insert avec Ã©volution du schema manuelle
    df_with_last_name = spark.createDataFrame(
        [
            (11, "Jack", "2023-01-09", "jack@example.com", "Derrick"),
            (12, "Lola", "2023-01-09", "lola@example.com", "Donovan"),
        ],
        ["id", "name", "signup_date", "email", "last_name"],
    )
    spark.sql(""" ALTER TABLE db.signup ADD COLUMN last_name STRING """)
    df_with_last_name.writeTo("db.signup").append()
    df_iceberg = spark.read.format("iceberg").load("db.signup").orderBy("id").show()

    # 6. Lister les snapshots
    snapshots_df = spark.sql(f"SELECT * FROM db.signup.snapshots").orderBy(
        "committed_at",
    )
    snapshots_df.show()

    # 7. Lire les donnÃ©es d'un snapshots (time travel)
    first_snapshot_id = snapshots_df.limit(1).collect()[0].snapshot_id
    spark.read.option("snapshot-id", first_snapshot_id).format("iceberg").load(
        "db.signup"
    ).orderBy("id").show()
```

```docker
version: "3.9"
name: iceberg-bucket

services:
  minio:
    image: minio/minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: MyUserTest1
      MINIO_ROOT_PASSWORD: MyUserTest1
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data

  createbuckets:
    image: quay.io/minio/mc:RELEASE.2025-03-12T17-29-24Z
    depends_on:
      - minio
    restart: on-failure
    entrypoint: >
      /bin/sh -c "
      sleep 5;
      /usr/bin/mc alias set dockerminio http://minio:9000 MyUserTest1 MyUserTest1;
      /usr/bin/mc mb dockerminio/local-bucket;
      exit 0;
      "

volumes:
  minio_data:
```
